# RawNetLite: Lightweight End-to-End Audio Deepfake Detection

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![PyTorch](https://img.shields.io/badge/framework-pytorch-yellow.svg)
![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)

This repository contains the official implementation of the paper:

> **End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation**  
> *Andrea Di Pierno, Luca Guarnera, Dario Allegra, Sebastiano Battiato*  
> In Proceedings of the **VERIMEDIA Workshop at IJCNN 2025**, Rome, Italy.

---

## ğŸ§  Overview

**RawNetLite** is a lightweight convolutional-recurrent model designed to detect audio deepfakes directly from raw waveforms, without relying on handcrafted features or large pretrained models.

The model is trained and evaluated under in-domain and cross-dataset scenarios using three public datasets: [FakeOrReal](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset), [AVSpoof2021](https://www.asvspoof.org/index2021.html), and [CodecFake](https://github.com/roger-tseng/CodecFake).

We introduce a training pipeline based on:

- **Raw waveform input**
- **Domain-mix learning**
- **Focal Loss optimization**
- **Waveform-level audio augmentations**

---

## ğŸ“„ Paper

If you use this code, please cite our paper:

```bibtex
@inproceedings{dipierno2025rawnetlite,
  title     = {End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation},
  author    = {Andrea Di Pierno and Luca Guarnera and Dario Allegra and Sebastiano Battiato},
  booktitle = {International Joint Conference on Neural Networks (IJCNN) - VERIMEDIA Workshop},
  year      = {2025}
}
```
## ğŸ“‚ Directory Structure
```bash
.
â”œâ”€â”€ models/                   # Pretrained models
    â”œâ”€â”€ rawnet_lite.pt                                      # Basic RawNetLite model
    â”œâ”€â”€ cross_domain_rawnet_lite.pt                         # Cross-domain RawNetLite model
    â”œâ”€â”€ cross_domain_focal_rawnet_lite.pt                   # Cross-domain RawNetLite with Focal Loss
    â”œâ”€â”€ triple_cross_domain_focal_rawnet_lite.pt            # Triple cross-domain RawNetLite with Focal Loss
    â”œâ”€â”€ augmented_triple_cross_domain_focal_rawnet_lite.pt  # Augmented triple cross-domain RawNetLite with Focal Loss
â”œâ”€â”€ .gitignore                # Git ignore file
â”œâ”€â”€ .gitattributes            # Git attributes file
â”œâ”€â”€ audio_preprocessor.py     # Audio preprocessing module
â”œâ”€â”€ AVSpoof_dataset.py        # AVSpoof PyTorch dataset
â”œâ”€â”€ CodecFake_dataset.py      # CodecFake PyTorch dataset
â”œâ”€â”€ focal_loss.py             # Focal loss implementation
â”œâ”€â”€ FOR_dataset.py            # FakeOrReal PyTorch dataset
â”œâ”€â”€ LICENSE                   # License file
â”œâ”€â”€ Mixed_dataset.py          # Mixed-domain PyTorch datasets
â”œâ”€â”€ RawNetLite.py             # Main model architecture
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ tester.py                 # Testing script
â””â”€â”€ trainer.py                  # Training script
```
## ğŸ›  Setup
1. Clone the repository:

```bash
git clone https://github.com/adipiz99/rawnetlite.git
cd RawNetLite
```
2. Install the required packages:

```bash
pip install -r requirements.txt
```
---

## ğŸ” Preprocessing

To preprocess your dataset into waveform tensors (.pt):

```bash
python audio_preprocessor.py \
    --csv_path metadata.csv \
    --input_dir path/to/audio \
    --output_root data/audio_processed/
```

This will create `real_processed/` and `fake_processed/` folders with normalized, trimmed, and resampled audio waveforms.

---

## ğŸ§ª Training & evaluation

To run the training script, set the parameters in `trainer.py` and use:

```bash
python trainer.py
```
The script outputs:
- Training loss, validation accuracy and F1 score
- Classification metrics (Precision, Recall, F1) for the validation set
- Equal Error Rate (EER) and threshold for the validation set
- A model trained following the specified parameters

To run the test bench and evaluate all models across all datasets, set the parameters in `tester.py` and use:

```bash
python tester.py
```

The script outputs:
- Classification metrics (Precision, Recall, F1)
- Equal Error Rate (EER) and threshold
- Support for FakeOrReal, AVSpoof2021, CodecFake, and mixed-domain evaluations

Please note that the training and testing scripts need to be run __using different data__, to avoid dataset overlapping.

---

## ğŸ¯ Pretrained Models

Pretrained have been released into the `models/` folder.

- `rawnet_lite.pt`: Basic RawNetLite model trained on the FakeOrReal dataset with BCE Loss.
- `cross_domain_rawnet_lite.pt`: Cross-domain RawNetLite model trained on the FOR dataset and the AVSpoof2021 dataset with BCE Loss.
- `cross_domain_focal_rawnet_lite.pt`: Cross-domain RawNetLite model trained on the FOR dataset and the AVSpoof2021 dataset with Focal Loss.
- `triple_cross_domain_focal_rawnet_lite.pt`: Triple cross-domain RawNetLite model trained on the FOR dataset, the AVSpoof2021 dataset, and the CodecFake dataset with Focal Loss.
- `augmented_triple_cross_domain_focal_rawnet_lite.pt`: Augmented triple cross-domain RawNetLite model trained on the FOR dataset, the AVSpoof2021 dataset, and the CodecFake dataset with Focal Loss and augmentation.

---

## ğŸ—‚ Datasets

This repository supports the following datasets:
- [FakeOrReal](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)
- [AVSpoof2021](https://www.asvspoof.org/index2021.html)
- [CodecFake](https://github.com/roger-tseng/CodecFake)

Each must be preprocessed using the provided script. Ensure correct splits for training and evaluation.

---

## âš– License

This project is licensed under the MIT License. See the `LICENSE` file for details.

---

## ğŸ“¬ Contact

If you have questions or find this project useful, feel free to contact us:

- Andrea Di Pierno â€” [andrea.dipierno@imtlucca.it](mailto:andrea.dipierno@imtlucca.it)

---

## ğŸ“Œ Acknowledgments

This study has been partially supported by SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union - NextGenerationEU
